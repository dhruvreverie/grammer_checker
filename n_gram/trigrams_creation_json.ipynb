{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcd47bc-ac43-481b-9206-ba28a8755f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 21:23:35.818563: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 21:23:35.818625: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 21:23:35.818637: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-03 21:23:35.824531: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import multiprocessing\n",
    "import json\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "import concurrent.futures\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787912f-b1c0-4926-b5ec-68bb573cef10",
   "metadata": {},
   "source": [
    "# Reading the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a014ab-61cb-4ba2-9f97-466c0ae2aa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8433, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_daily.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fa224f-5c24-4340-8822-4548ab6324c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['correct_sentence'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103767f-ffc3-4273-b735-8f6f9fdd6da8",
   "metadata": {},
   "source": [
    "### Preprocessing the datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de62cce3-e0f2-496a-a4f2-60cf8c4a30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_hindi(data, column_name):\n",
    "    new_column = 'pre_process_' + str(column_name)\n",
    "    \n",
    "    # Create a copy of the original column\n",
    "    data[new_column] = data[column_name].copy()\n",
    "    \n",
    "    # Remove URLs\n",
    "    data[new_column] = data[new_column].apply(lambda x: re.sub(r'(https|http)?:\\/\\/\\S+|www\\.\\S+', ' ', str(x)))\n",
    "    print(\"Removed URLs\")\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    data[new_column] = data[new_column].apply(lambda x: re.sub(r'<.*?>', ' ', str(x)))\n",
    "    print(\"Removed HTML Tags\")\n",
    "    \n",
    "    # Remove \\n\\t by space\n",
    "    data[new_column] = data[new_column].apply(lambda x: re.sub(r'[\\n\\r\\t]', ' ', str(x)))\n",
    "    print(\"Removed tabs and new lines\")\n",
    "    \n",
    "    # Define Hindi characters range and additional characters to keep\n",
    "    hindi_pattern = r'[\\u0900-\\u097F\\u0020\\u0964\\u0965\\u0966-\\u096F]'\n",
    "    \n",
    "    # Keep only Hindi characters and specified punctuation\n",
    "    data[new_column] = data[new_column].apply(lambda x: ''.join(re.findall(hindi_pattern, str(x))))\n",
    "    print(\"Kept only Hindi characters and specified punctuation\")\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    data[new_column] = data[new_column].apply(lambda x: re.sub(r'\\s+', ' ', str(x)).strip())\n",
    "    print(\"Removed extra spaces\")\n",
    "    \n",
    "    data.dropna(subset=[new_column], inplace=True)\n",
    "    data.drop_duplicates(subset=[new_column], inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2dc154-d1f3-4084-a74b-b2d0a1f9722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed URLs\n",
      "Removed HTML Tags\n",
      "Removed tabs and new lines\n",
      "Kept only Hindi characters and specified punctuation\n",
      "Removed extra spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>pre_process_correct_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>जगदीश के नए ईमेल का जवाब दें</td>\n",
       "      <td>जगदीश के नए ईमेल का जवाब दें</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>मुझे एक चरनी चाहिए</td>\n",
       "      <td>मुझे एक चरनी चाहिए</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walmart को tweet भेजो</td>\n",
       "      <td>को भेजो</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>फ़ोन नंबर 6180028469 और MPIN 8519 का उपयोग करक...</td>\n",
       "      <td>फ़ोन नंबर और का उपयोग करके पोर्टल पर लॉग इन करें</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olly ओवन में आलू को किस टेंपरेचर पर पकाएं</td>\n",
       "      <td>ओवन में आलू को किस टेंपरेचर पर पकाएं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>बाद के लिए ईमेल सहेजें</td>\n",
       "      <td>बाद के लिए ईमेल सहेजें</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>बसना का क्या मतलब है</td>\n",
       "      <td>बसना का क्या मतलब है</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>राज्य तमिल नाडु में मेडिकल दूकान खोजने के लिए ...</td>\n",
       "      <td>राज्य तमिल नाडु में मेडिकल दूकान खोजने के लिए ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>मुझे कल सुबह साढ़े पांच बजे का अलार्म चाहिए</td>\n",
       "      <td>मुझे कल सुबह साढ़े पांच बजे का अलार्म चाहिए</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>अरे olly क्या समय हो गया है</td>\n",
       "      <td>अरे क्या समय हो गया है</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7861 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       correct_sentence  \\\n",
       "0                          जगदीश के नए ईमेल का जवाब दें   \n",
       "1                                    मुझे एक चरनी चाहिए   \n",
       "2                                 walmart को tweet भेजो   \n",
       "3     फ़ोन नंबर 6180028469 और MPIN 8519 का उपयोग करक...   \n",
       "4             olly ओवन में आलू को किस टेंपरेचर पर पकाएं   \n",
       "...                                                 ...   \n",
       "7856                             बाद के लिए ईमेल सहेजें   \n",
       "7857                               बसना का क्या मतलब है   \n",
       "7858  राज्य तमिल नाडु में मेडिकल दूकान खोजने के लिए ...   \n",
       "7859        मुझे कल सुबह साढ़े पांच बजे का अलार्म चाहिए   \n",
       "7860                        अरे olly क्या समय हो गया है   \n",
       "\n",
       "                           pre_process_correct_sentence  \n",
       "0                          जगदीश के नए ईमेल का जवाब दें  \n",
       "1                                    मुझे एक चरनी चाहिए  \n",
       "2                                               को भेजो  \n",
       "3      फ़ोन नंबर और का उपयोग करके पोर्टल पर लॉग इन करें  \n",
       "4                  ओवन में आलू को किस टेंपरेचर पर पकाएं  \n",
       "...                                                 ...  \n",
       "7856                             बाद के लिए ईमेल सहेजें  \n",
       "7857                               बसना का क्या मतलब है  \n",
       "7858  राज्य तमिल नाडु में मेडिकल दूकान खोजने के लिए ...  \n",
       "7859        मुझे कल सुबह साढ़े पांच बजे का अलार्म चाहिए  \n",
       "7860                             अरे क्या समय हो गया है  \n",
       "\n",
       "[7861 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_hindi(df, 'correct_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26057f3a-193c-4416-b9da-90e3c500a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the rows where the length is less than 3\n",
    "df = df[df['pre_process_correct_sentence'].apply(lambda x: len(x.split()) >= 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1396892-d4bd-4e67-9b81-71c2b59a6883",
   "metadata": {},
   "source": [
    "### Masking the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827a76e9-e67d-4dd3-9694-db0289d3402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model for NER Classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NER_model\")\n",
    "model = ORTModelForTokenClassification.from_pretrained('NER_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b41ac6-8af7-4e66-941e-79118e5066f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline('token-classification', model=model, tokenizer=tokenizer, aggregation_strategy='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ecb8f9-e0fd-4890-b2c2-dd1059d0f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get the predictions\n",
    "def get_predictions(sentence):\n",
    "    \"\"\"\n",
    "    Process a single sentence and replace named entities with 'MASK'.\n",
    "    \n",
    "    :param sentence: A string containing the sentence to process\n",
    "    :return: A string with named entities replaced by 'MASK'\n",
    "    \"\"\"\n",
    "    # Process the sentence using the NER pipeline\n",
    "    entities = ner_pipeline(sentence)\n",
    "    \n",
    "    # Create masked sentence\n",
    "    masked_sentence = sentence\n",
    "    for item in sorted(entities, key=lambda x: x['start'], reverse=True):\n",
    "        start, end = item['start'], item['end']\n",
    "        masked_sentence = masked_sentence[:start] + \"MASK\" + masked_sentence[end:]\n",
    "    \n",
    "    return masked_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a26daebb-bbb3-4c50-8048-33862da49087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    return get_predictions(row['pre_process_correct_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7624af52-b8d6-4270-ae8a-e76a2f95873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7568/7568 [00:23<00:00, 326.02it/s]\n",
      "/tmp/ipykernel_21007/371658027.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Masked_sentence'] = list(tqdm(executor.map(process_row, [row for _, row in df.iterrows()]), total=len(df)))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Read the DataFrame\n",
    "    # df = pd.read_csv(\"hindi_wiki.csv\")\n",
    "\n",
    "    # Get the number of CPUs and set the number of workers to use\n",
    "    num_cpus = os.cpu_count()\n",
    "    num_workers = int(num_cpus * 0.8)\n",
    "    \n",
    "    # Apply the function to the DataFrame using multiprocessing\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Create a progress bar\n",
    "        tqdm.pandas(desc=\"Processing\")\n",
    "        \n",
    "        # Apply the function to each row in parallel\n",
    "        df['Masked_sentence'] = list(tqdm(executor.map(process_row, [row for _, row in df.iterrows()]), total=len(df)))\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    # df.to_csv(\"hindi_wiki.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f4483-fb7f-404a-9f5c-98ca75b12a28",
   "metadata": {},
   "source": [
    "### Generate trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dce1acb-1d51-4711-8482-08d266afcb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigrams(text, word_to_combine):\n",
    "    try:\n",
    "        words = text.split()\n",
    "        output = []\n",
    "        for i in range(len(words)- word_to_combine+1):\n",
    "            output.append(tuple(words[i:i+word_to_combine]))\n",
    "        return output\n",
    "    except TypeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc309088-24dc-47ed-89c3-a7aeea667e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 7568/7568 [00:00<00:00, 244135.46it/s]\n",
      "/tmp/ipykernel_21007/2208722436.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['trigrams'] = df['Masked_sentence'].progress_apply(lambda x: generate_trigrams(x, 3))\n"
     ]
    }
   ],
   "source": [
    "df['trigrams'] = df['Masked_sentence'].progress_apply(lambda x: generate_trigrams(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85fc2e8f-734d-416d-a0a8-ff1bcca6e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_list = df['trigrams'].tolist()\n",
    "flattened_list = list({item for sublist in trigram_list for item in sublist})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf08495-7904-4c16-9cc2-74c8ae2dadc8",
   "metadata": {},
   "source": [
    "## Saving the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc3b703-3ad3-4e4c-a77a-079e600a5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert list of tuples to list of lists\n",
    "# data_list_of_lists = [list(item) for item in flattened_list]\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = 'data.json'\n",
    "\n",
    "# # Write the list of lists to a JSON file\n",
    "# with open(file_path, 'w') as json_file:\n",
    "#     json.dump(data_list_of_lists, json_file)\n",
    "\n",
    "# print(f'Data has been saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8cc0cde-0047-4ba4-9764-c4708fbe8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to read the JSON file\n",
    "# def read_json(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as json_file:\n",
    "#             data_list_of_lists = json.load(json_file)\n",
    "#         return [tuple(item) for item in data_list_of_lists]\n",
    "#     except FileNotFoundError:\n",
    "#         return []\n",
    "\n",
    "# # Function to write data to the JSON file\n",
    "# def write_json(file_path, data):\n",
    "#     data_list_of_lists = [list(item) for item in data]\n",
    "#     with open(file_path, 'w') as json_file:\n",
    "#         json.dump(data_list_of_lists, json_file)\n",
    "\n",
    "# # Function to append new data to the JSON file\n",
    "# def append_to_json(file_path, new_data):\n",
    "#     # Read existing data\n",
    "#     data = read_json(file_path)\n",
    "    \n",
    "#     # Append new data (if it's not already present)\n",
    "#     for item in tqdm(new_data):\n",
    "#         if item not in data:\n",
    "#             data.append(item)\n",
    "    \n",
    "#     # Write the updated data back to the JSON file\n",
    "#     write_json(file_path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561ceee3-c6d8-4f6b-9fc0-fb76d62b5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc5f8be-186c-43bb-9771-2f20f858b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Append new data to the JSON file\n",
    "# append_to_json(file_path, flattened_list)\n",
    "\n",
    "# print(f'New data has been appended to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3adbcc-e3ff-4c6a-a6db-420561dcf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data_list_of_lists = json.load(json_file)\n",
    "        return [tuple(item) for item in data_list_of_lists]\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "def write_json(file_path, data):\n",
    "    data_list_of_lists = [list(item) for item in data]\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data_list_of_lists, json_file)\n",
    "\n",
    "def process_chunk(args):\n",
    "    chunk, existing_data = args\n",
    "    return [item for item in chunk if item not in existing_data]\n",
    "\n",
    "def append_to_json(file_path, new_data):\n",
    "    # Read existing data\n",
    "    existing_data = set(read_json(file_path))\n",
    "    \n",
    "    # Determine the number of CPU cores to use (80% of available cores)\n",
    "    num_cores = max(1, int(multiprocessing.cpu_count() * 0.8))\n",
    "    \n",
    "    # Split new_data into chunks\n",
    "    chunk_size = max(1, len(new_data) // num_cores)\n",
    "    chunks = [new_data[i:i + chunk_size] for i in range(0, len(new_data), chunk_size)]\n",
    "    \n",
    "    # Prepare arguments for process_chunk\n",
    "    args = [(chunk, existing_data) for chunk in chunks]\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    with multiprocessing.Pool(num_cores) as pool:\n",
    "        results = list(tqdm(pool.imap(process_chunk, args), \n",
    "                            total=len(chunks), desc=\"Processing chunks\"))\n",
    "    \n",
    "    # Combine results\n",
    "    new_items = [item for sublist in results for item in sublist]\n",
    "    \n",
    "    # Append new items to existing data\n",
    "    updated_data = list(existing_data) + new_items\n",
    "    \n",
    "    # Write the updated data back to the JSON file\n",
    "    write_json(file_path, updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9716b9d-1f5d-47d3-9a41-35f54a1455a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 39/39 [00:00<00:00, 806.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'data1.json'\n",
    "    # new_data = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]  # Example new data\n",
    "    append_to_json(file_path, flattened_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfeb44-183c-49df-889e-10779ce6888e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
